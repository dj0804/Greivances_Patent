# Hostel Grievance Urgency Classification & Routing System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.10+](https://img.shields.io/badge/TensorFlow-2.10+-orange.svg)](https://www.tensorflow.org/)

**An intelligent multi-stage decision pipeline for automated complaint prioritization combining deep learning, semantic vector search, temporal pattern analysis, and density-based clustering.**

---

## Table of Contents

- [Overview](#overview)
- [Problem Statement](#problem-statement)
- [System Features](#system-features)
- [Architecture Summary](#architecture-summary)
- [Technology Stack](#technology-stack)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Documentation](#documentation)
- [API Usage](#api-usage)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

The **Hostel Grievance Urgency Classification & Routing System** addresses the challenge of triaging large volumes of complaints in institutional hostel environments. Unlike traditional text classifiers that operate on content alone, this system implements a **hybrid multi-signal architecture** that jointly analyzes:

1. **Semantic Similarity** (transformer-based embeddings + vector database)
2. **Inferred Urgency** (CNN-BiLSTM deep learning model)
3. **Temporal Frequency Patterns** (JSON-based temporal encoding with burst detection)
4. **Historical Resolution Feedback** (self-calibrating urgency thresholds)
5. **Cluster Density** (HDBSCAN clustering over embeddings for systemic issue detection)

The system is **not a simple classifier**â€”it is a **multi-stage decision pipeline** with modular components that produce context-aware, adaptive priority scores suitable for real-world deployment in high-volume complaint management scenarios.

---

## Problem Statement

### Challenges in Manual Complaint Management

Institutional hostels receive **hundreds to thousands** of complaints daily across categories:
- **Maintenance**: Water heaters, ACs, electrical issues
- **Safety**: Door locks, lighting, security concerns
- **Cleanliness**: Room cleaning, common area maintenance
- **Food**: Dining hall quality, hygiene issues
- **Administrative**: Fee issues, room allocation

**Problems with Manual Triage**:
1. **High Volume**: Staff cannot read and prioritize 500+ daily complaints
2. **Context Blindness**: A single "AC broken" complaint may be low urgency, but 30 similar complaints indicate a systemic HVAC failure
3. **Time Sensitivity**: A door lock complaint at 2 AM is far more urgent than the same complaint at 2 PM
4. **Subjective Judgment**: Different staff members prioritize differently, causing inconsistency
5. **Delayed Response**: Critical issues buried in noise lead to escalation and student dissatisfaction

### Why Existing Solutions Fall Short

| Approach | Limitation |
|----------|------------|
| **Keyword Matching** | Easily gamed; misses context |
| **Rule-Based Systems** | Brittle; requires constant manual updates |
| **Pure ML Classifiers** | Context-blind; no systemic issue detection |
| **Sentiment Analysis** | Urgency â‰  sentiment; misses technical failures |

**Our Solution**: A **hybrid decision architecture** that combines strengths of ML, vector search, temporal analysis, and rule-based logic to produce robust, explainable priority scores.

---

## System Features

### Core Capabilities

âœ… **Intelligent Preprocessing**
- Emotion-preserving text cleaning (retains ALL CAPS, excessive punctuation, emojis as urgency signals)
- Urgency-aware tokenization (preserves negations: "not", "no", "never")
- Temporal feature extraction (hour, day, peak times, frequency patterns)

âœ… **Hybrid Deep Learning Architecture**
- **CNN Layer**: Extracts local n-gram patterns indicative of urgency
- **BiLSTM Layer**: Captures long-range contextual dependencies
- **Dense Head**: Outputs urgency probability distribution

âœ… **Semantic Vector Search**
- Transformer-based sentence embeddings (Sentence-BERT / Universal Sentence Encoder)
- Vector database for fast similarity search
- Identifies similar complaints for cluster analysis

âœ… **Density-Based Clustering**
- HDBSCAN automatic clustering (no manual cluster count)
- Spatial density: Semantic similarity within clusters
- Temporal density: Complaint frequency over time
- **Systemic Issue Detection**: High-density clusters indicate infrastructure failures

âœ… **Temporal Frequency Analysis**
- JSON-based encoding of time features
- Burst detection (2x spike in complaint rate â†’ outbreak)
- Time-of-day adjustments (night, peak hours, weekends)

âœ… **Historical Feedback Loop**
- Tracks actual resolution times vs. predicted urgency
- Self-calibrating weights based on SLA performance
- **No retraining required** for adaptation

âœ… **Multi-Signal Decision Fusion**
- Weighted aggregation of 5 signals (model, cluster, temporal, history, semantic)
- Adaptive weight tuning based on system state
- Override rules for safety-critical edge cases

âœ… **RESTful API**
- FastAPI endpoint for real-time predictions
- Batch processing support
- Explainable outputs with decision breakdown

---

## Architecture Summary

### High-Level Flow

```
Raw Complaint + Metadata
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PREPROCESSING     â”‚
â”‚  â€¢ Clean Text      â”‚
â”‚  â€¢ Tokenize        â”‚
â”‚  â€¢ Temporal Extractâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚BRANCH Aâ”‚  â”‚ BRANCH B â”‚
â”‚CNN-BiLSTMâ”‚ â”‚ Semantic â”‚
â”‚Urgency â”‚  â”‚Embeddingsâ”‚
â”‚Predict â”‚  â”‚+ Cluster â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚DECISION ENGINEâ”‚
    â”‚Multi-Signal  â”‚
    â”‚Fusion        â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    Priority + Routing
```

**Parallel Processing**: Branch A (CNN-BiLSTM) and Branch B (Embeddings + Clustering) run concurrently for 1.5x speedup.

### Decision Fusion Formula

```
Priority = Î±Â·U_model + Î²Â·D_cluster + Î³Â·T_temporal + Î´Â·H_history + ÎµÂ·S_semantic

Default Weights:
  Î± = 0.40  (Model Urgency)
  Î² = 0.25  (Cluster Density - systemic issues)
  Î³ = 0.15  (Temporal Context)
  Î´ = 0.10  (Historical Calibration)
  Îµ = 0.10  (Semantic Similarity to Critical Cases)
```

**See [docs/architecture.md](docs/architecture.md) for detailed component architecture.**

---

## Technology Stack

| Component | Technology |
|-----------|------------|
| **Deep Learning** | TensorFlow/Keras 2.10+ |
| **Word Embeddings** | FastText (300-dim pretrained) |
| **Sentence Embeddings** | Sentence-BERT / Universal Sentence Encoder |
| **NLP Processing** | spaCy, NLTK, emoji, contractions |
| **Clustering** | HDBSCAN (density-based) |
| **Vector Database** | FAISS |
| **API Framework** | FastAPI + Uvicorn |
| **Data Processing** | NumPy, Pandas, scikit-learn |
| **Serialization** | Pickle, JSON |

**System Requirements**:
- Python 3.8+
- 8GB RAM (16GB recommended for training)
- GPU optional (10x training speedup with CUDA)

---

## Project Structure

```
hostel-patent/
â”‚
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ REFACTORING_SUMMARY.md             # Development history
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ run_tests.sh                       # Test automation script
â”‚
â”œâ”€â”€ api/                               # FastAPI application
â”‚   â”œâ”€â”€ main.py                        # API server entry point
â”‚   â”œâ”€â”€ routes.py                      # Endpoint definitions
â”‚   â””â”€â”€ schemas.py                     # Pydantic request/response models
â”‚
â”œâ”€â”€ data/                              # Data storage (Git LFS recommended)
â”‚   â”œâ”€â”€ raw/                           # Immutable raw data
â”‚   â”‚   â”œâ”€â”€ complaints/                # Original complaint texts
â”‚   â”‚   â””â”€â”€ metadata/                  # Timestamps, hostel IDs, etc.
â”‚   â”œâ”€â”€ processed/                     # Transformed artifacts
â”‚   â”‚   â”œâ”€â”€ tokenized/                 # Clean tokens + vocabulary
â”‚   â”‚   â”œâ”€â”€ embeddings/                # FastText sequences + sentence embeddings
â”‚   â”‚   â””â”€â”€ temporal_json/             # Temporal feature encodings
â”‚   â””â”€â”€ external/                      # Pretrained models
â”‚       â”œâ”€â”€ cc.en.300.bin              # FastText model (download separately)
â”‚       â””â”€â”€ sbert_model/               # Sentence-BERT checkpoint
â”‚
â”œâ”€â”€ docs/                              # Comprehensive documentation
â”‚   â”œâ”€â”€ architecture.md                # System architecture details
â”‚   â”œâ”€â”€ data_flow.md                   # Data lifecycle & pipeline
â”‚   â”œâ”€â”€ decision_logic.md              # Decision engine logic (patent-worthy)
â”‚   â””â”€â”€ preprocessing.md               # (if exists) Text preprocessing details
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks for exploration
â”‚
â”œâ”€â”€ outputs/                           # Training and inference outputs
â”‚   â”œâ”€â”€ models/                        # Trained model checkpoints
â”‚   â”‚   â”œâ”€â”€ cnn_bilstm_best.h5
â”‚   â”‚   â”œâ”€â”€ tokenizer.pkl
â”‚   â”‚   â””â”€â”€ label_encoder.pkl
â”‚   â”œâ”€â”€ logs/                          # Training logs & TensorBoard
â”‚   â””â”€â”€ reports/                       # Inference predictions, cluster analysis
â”‚
â”œâ”€â”€ scripts/                           # Reproducible execution scripts
â”‚   â”œâ”€â”€ train_model.py                 # Model training pipeline
â”‚   â”œâ”€â”€ build_embeddings.py            # Vector database construction
â”‚   â””â”€â”€ run_inference.py               # Batch/single inference
â”‚
â”œâ”€â”€ src/                               # Core library code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                      # Centralized configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/                 # Text preprocessing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ clean_text.py              # Emotion-preserving cleaning
â”‚   â”‚   â”œâ”€â”€ tokenizer.py               # Urgency-aware tokenization
â”‚   â”‚   â””â”€â”€ temporal_encoder.py        # Temporal feature extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/                    # Embedding generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ semantic_embedder.py       # FastText encoder
â”‚   â”‚   â””â”€â”€ vector_store.py            # Vector database interface
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                        # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cnn.py                     # CNN model
â”‚   â”‚   â”œâ”€â”€ bilstm.py                  # BiLSTM model
â”‚   â”‚   â”œâ”€â”€ cnn_bilstm.py              # Hybrid CNN-BiLSTM
â”‚   â”‚   â””â”€â”€ dense_head.py              # Classification head
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                      # Training pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py                   # Training logic
â”‚   â”‚   â”œâ”€â”€ loss.py                    # Custom loss functions
â”‚   â”‚   â””â”€â”€ metrics.py                 # Evaluation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                     # Prediction pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ predict.py                 # Inference wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ decision_engine/               # Decision fusion layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ urgency_logic.py           # Multi-signal fusion logic
â”‚   â”‚
â”‚   â””â”€â”€ utils/                         # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ helpers.py                 # General helpers
â”‚       â””â”€â”€ logger.py                  # Logging configuration
â”‚
â””â”€â”€ tests/                             # Unit and integration tests
    â”œâ”€â”€ test_preprocessing.py
    â”œâ”€â”€ test_models.py
    â””â”€â”€ test_inference.py
```

---

## Installation

### Prerequisites

- **Python 3.8+**
- **pip** or **conda** for package management
- **Git LFS** (optional, for large model files)

### Step 1: Clone Repository

```bash
git clone https://github.com/dj0804/Greivances_Patent.git
cd hostel-patent
```

### Step 2: Create Virtual Environment

```bash
# Using venv
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# OR using conda
conda create -n hostel-patent python=3.8
conda activate hostel-patent
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt** includes:
- `tensorflow>=2.10.0`
- `spacy`, `nltk`, `gensim`, `fasttext`
- `scikit-learn`, `hdbscan`
- `fastapi`, `uvicorn`, `pydantic`
- `numpy`, `pandas`, `tqdm`

### Step 4: Download External Models

**spaCy Language Model**:
```bash
python -m spacy download en_core_web_sm
```

**FastText Pretrained Embeddings**:
```bash
# Download from https://fasttext.cc/docs/en/crawl-vectors.html
# Example: English 300-dim
wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
gunzip cc.en.300.bin.gz
mv cc.en.300.bin data/external/
```

**Sentence-BERT Model** (optional, for semantic embeddings):
```python
# Will auto-download on first use via transformers library
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
model.save('data/external/sbert_model')
```

### Step 5: Verify Installation

```bash
python -c "import tensorflow as tf; import spacy; print('TensorFlow:', tf.__version__); print('spaCy:', spacy.__version__)"
```

---

## Quick Start

### 1. Prepare Training Data

Place complaint data in `data/raw/complaints/` (supports `.txt`, `.csv`, `.json`):

**Example format** (`data/raw/complaints/batch_001.json`):
```json
[
  {
    "id": "C_001",
    "text": "Water heater broken in room 204, no hot water since morning!!!",
    "timestamp": "2026-02-17T08:30:00Z",
    "label": "High",
    "metadata": {"hostel_id": "H-101", "room": "204"}
  },
  {
    "id": "C_002",
    "text": "Light bulb needs replacement in corridor",
    "timestamp": "2026-02-17T10:15:00Z",
    "label": "Low",
    "metadata": {"hostel_id": "H-101", "corridor": "A"}
  }
]
```

### 2. Train the Model

```bash
python scripts/train_model.py \
    --data data/raw/complaints/batch_001.json \
    --model-type cnn_bilstm \
    --embedding-path data/external/cc.en.300.bin \
    --epochs 50 \
    --batch-size 32 \
    --output outputs/models/
```

**Output**:
- `outputs/models/cnn_bilstm_best.h5` (trained weights)
- `outputs/models/tokenizer.pkl` (fitted tokenizer)
- `outputs/logs/training_history.csv` (metrics)

**Training takes ~30-60 minutes** (5000 samples, GPU recommended).

### 3. Build Embedding Database

```bash
python scripts/build_embeddings.py \
    --input data/raw/complaints/batch_001.json \
    --model-path data/external/sbert_model \
    --output data/processed/embeddings/vector_store.pkl \
    --run-clustering
```

**Output**:
- `data/processed/embeddings/vector_store.pkl` (searchable vector DB)
- `data/processed/embeddings/cluster_analysis.html` (visualization)

### 4. Run Inference

**Single Complaint**:
```bash
python scripts/run_inference.py \
    --model outputs/models/cnn_bilstm_best.h5 \
    --vector-store data/processed/embeddings/vector_store.pkl \
    --text "The AC in my room has been making loud noises all night, can't sleep!!!" \
    --output outputs/reports/prediction.json
```

**Batch Inference**:
```bash
python scripts/run_inference.py \
    --model outputs/models/cnn_bilstm_best.h5 \
    --vector-store data/processed/embeddings/vector_store.pkl \
    --input data/raw/complaints/new_batch.json \
    --output outputs/reports/batch_predictions.json
```

**Output** (`prediction.json`):
```json
{
  "priority_score": 0.712,
  "urgency_level": "High",
  "routing": "Priority Maintenance Queue",
  "sla_hours": 2,
  "cluster_id": 8,
  "explanation": "High urgency due to model prediction (0.78), night-time filing boost (0.15), and cluster density (0.65 - 12 similar AC complaints this week)"
}
```

### 5. Start API Server

```bash
cd api
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**API accessible at**: `http://localhost:8000`

**Interactive docs**: `http://localhost:8000/docs`

---

## Documentation

Comprehensive technical documentation available in `docs/`:

| Document | Description |
|----------|-------------|
| [architecture.md](docs/architecture.md) | **System Architecture**: Detailed component design, CNN/BiLSTM internals, parallel processing, embedding generation, HDBSCAN clustering |
| [data_flow.md](docs/data_flow.md) | **Data Lifecycle**: Complete data pipeline from raw ingestion to final output, preprocessing steps, embedding construction, inference flow |
| [decision_logic.md](docs/decision_logic.md) | **Decision Engine**: Multi-signal fusion formula, cluster density calculation, temporal frequency analysis, historical calibration, patent-worthy novel aspects |
| [preprocessing.md](docs/preprocessing.md) | **Preprocessing Details**: Text cleaning, tokenization, temporal encoding (if exists) |

**Start Here**: [docs/architecture.md](docs/architecture.md) for system overview.

---

## API Usage

### Endpoint: POST `/predict`

**Request**:
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Water heater broken in room 204, no hot water since morning!!!",
    "timestamp": "2026-02-17T08:30:00Z",
    "metadata": {
      "hostel_id": "H-101",
      "room": "204"
    }
  }'
```

**Response**:
```json
{
  "complaint_id": "generated_uuid",
  "priority_score": 0.668,
  "urgency_level": "High",
  "routing_recommendation": "Priority Maintenance Queue",
  "sla_hours": 2,
  "requires_immediate_action": false,
  "cluster_id": 7,
  "similar_complaints_count": 15,
  "decision_breakdown": {
    "model_urgency": 0.74,
    "cluster_density": 0.72,
    "temporal_boost": 0.30,
    "historical_calibration": 0.75,
    "semantic_similarity": 0.35
  },
  "explanation": "High priority due to model prediction (0.74), systemic issue detected (cluster #7 with 15 complaints in 3 days, density 0.72), peak hour filing (0.30), and historical pattern of delayed resolution (0.75)"
}
```

### Endpoint: POST `/batch-predict`

**Request**:
```bash
curl -X POST "http://localhost:8000/batch-predict" \
  -H "Content-Type: application/json" \
  -d '{
    "complaints": [
      {"text": "AC broken", "timestamp": "2026-02-17T08:00:00Z"},
      {"text": "WiFi slow", "timestamp": "2026-02-17T09:00:00Z"}
    ]
  }'
```

**See [api/routes.py](api/routes.py) for complete API specification.**

---

## How It Works: Key Algorithms

### 1. CNN-BiLSTM Urgency Model

**Input**: Token sequence `[tâ‚, tâ‚‚, ..., tâ‚™]`  
**Process**:
1. Embed tokens using FastText: `E = [eâ‚, eâ‚‚, ..., eâ‚™]` (each `eáµ¢ âˆˆ â„Â³â°â°`)
2. Apply 1D convolution (kernel size 5) to extract local patterns
3. Feed into Bidirectional LSTM to capture context
4. Dense head outputs `[P(Low), P(Medium), P(High)]`

**Training**: Categorical cross-entropy loss, Adam optimizer, early stopping on validation loss

### 2. HDBSCAN Clustering for Systemic Issue Detection

**Input**: Sentence embeddings from vector database  
**Process**:
1. Run HDBSCAN (min_cluster_size=5) to auto-identify clusters
2. Calculate **spatial density**: `1 / avg_intra_cluster_distance`
3. Calculate **temporal density**: `complaints_per_hour` within cluster
4. Combined density score: `0.6 Ã— spatial + 0.4 Ã— temporal`

**Output**: Cluster ID, density score, outlier flag

**Insight**: High-density clusters indicate recurring/systemic issues â†’ boost priority

### 3. Temporal Frequency Burst Detection

**Input**: Complaint timestamp + historical complaint rate  
**Process**:
1. Count complaints in last 24 hours: `C_24h`
2. Retrieve historical average: `C_avg`
3. If `C_24h > 2 Ã— C_avg`: Burst detected â†’ `+0.20` boost
4. If `C_24h > 1.5 Ã— C_avg`: Elevated frequency â†’ `+0.10` boost

**Example**: Normal = 5/day, Today = 15/day â†’ 3x spike â†’ Burst boost applied

### 4. Historical Resolution Feedback Loop

**Input**: Past complaints similar to current complaint (semantic search)  
**Process**:
1. Retrieve resolution times of top-10 similar cases
2. Calculate `avg_resolution_time`
3. Compare to SLA target for predicted urgency level
4. If `avg_resolution_time > 1.5 Ã— SLA`: Upward calibration (`H_history = 0.80`)
5. If `avg_resolution_time < 0.8 Ã— SLA`: Downward calibration (`H_history = 0.30`)

**Result**: System self-corrects without retraining

---

## Contributing

We welcome contributions! Areas of interest:

1. **Model Improvements**: Experiment with Transformer encoders (BERT, RoBERTa) for urgency classification
2. **Clustering Algorithms**: Compare HDBSCAN with OPTICS, DBSCAN variants
3. **Explainability**: Integrate attention visualization for model predictions
4. **Testing**: Expand unit test coverage (currently ~60%)
5. **Documentation**: Add tutorials, use-case examples

**Steps to Contribute**:
1. Fork repository
2. Create feature branch: `git checkout -b feature/your-feature`
3. Commit changes: `git commit -m "Add feature X"`
4. Push to branch: `git push origin feature/your-feature`
5. Open Pull Request

**Code Style**: Follow PEP 8, use type hints, docstrings for all functions.

---

## Testing

**Run all tests**:
```bash
bash run_tests.sh
```

**Run specific test suite**:
```bash
python -m pytest tests/test_preprocessing.py -v
python -m pytest tests/test_models.py -v
python -m pytest tests/test_inference.py -v
```

**Coverage report**:
```bash
pytest --cov=src --cov-report=html
open htmlcov/index.html
```

---

## Acknowledgments

- **FastText**: Facebook AI Research for pretrained word embeddings
- **Sentence-BERT**: Nils Reimers for semantic sentence embeddings
- **HDBSCAN**: Leland McInnes for robust clustering algorithm
- **TensorFlow/Keras**: Google for deep learning framework
- **spaCy**: Explosion AI for NLP pipeline

## Project Status

ðŸš€ **Status**: Active Development  
ðŸ“… **Last Updated**: February 2026  
ðŸŽ¯ **Current Version**: v1.2.0  
âœ… **Production Ready**: Yes (with appropriate testing for your use case)

---

**Built with â¤ï¸ for smarter hostel management**

```bash
python api/main.py
```

or

```bash
uvicorn api.main:app --reload
```

Visit `http://localhost:8000/docs` for interactive API documentation.

## Usage Examples

### Python API

```python
from src.inference import GrievancePredictor
from src.decision_engine import UrgencyDecisionEngine
from datetime import datetime

# Initialize predictor
predictor = GrievancePredictor(
    model_path="outputs/models/best_model.h5"
)

# Make prediction
result = predictor.predict(
    complaint_text="The water heater is broken!!!",
    timestamp=datetime.now(),
    hostel_id="H-101"
)

print(f"Urgency: {result['urgency_level']}")
print(f"Confidence: {result['confidence']:.2f}")
```

### REST API

```bash
curl -X POST "http://localhost:8000/api/v1/predict" \\
     -H "Content-Type: application/json" \\
     -d '{
       "complaint_text": "The water heater is broken!!!",
       "timestamp": "2026-02-17T18:30:00",
       "hostel_id": "H-101"
     }'
```

## Model Architecture

### Text Processing Pipeline
1. **Cleaning**: Caps detection, emoji conversion, punctuation analysis
2. **Tokenization**: Lemmatization with custom stop word handling
3. **Embedding**: FastText word vectors (300-dim)

### Neural Network
- **Input**: Tokenized sequences (max length 100)
- **Embedding Layer**: Pretrained FastText weights
- **CNN Layer**: 128 filters, kernel size 5
- **BiLSTM Layers**: 64 units with dropout
- **Dense Layers**: 128 â†’ 64 â†’ 3 (urgency classes)

### Decision Engine
Rule-based adjustments based on:
- Temporal context (night/peak hours/weekend)
- Critical keywords (emergency, urgent, etc.)
- Complaint history (repeated issues)

## Development

### Running Tests

```bash
# Run all tests
python -m pytest tests/

# Run specific test file
python -m pytest tests/test_preprocessing.py

# With coverage
python -m pytest --cov=src tests/
```

## Documentation

See the `docs/` directory for detailed documentation:
- [architecture.md](docs/architecture.md) - System architecture
- [data_flow.md](docs/data_flow.md) - Data processing pipeline
- [decision_logic.md](docs/decision_logic.md) - Decision engine rules


## Acknowledgments

- FastText pretrained embeddings
- spaCy for NLP processing
- TensorFlow/Keras for deep learning
