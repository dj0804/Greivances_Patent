{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2b646-e4a7-4afc-8c87-02b25b5a48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e17158-9956-4f0e-b40c-3cbf67cb3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50          # max tokens per complaint\n",
    "EMBED_DIM = 300       # FastText dimension\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d65df-8125-4034-96cf-0a0b2b2831ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEncoder:\n",
    "    def __init__(self, model):\n",
    "        self.model = model  # gensim fasttext model\n",
    "\n",
    "    def encode_tokens(self, tokens):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.model:\n",
    "                vectors.append(self.model[token])\n",
    "            else:\n",
    "                vectors.append(np.zeros(EMBED_DIM))\n",
    "        return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a22a7d-61fc-4a03-a08b-c49bd7e6f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(embedding_matrix, max_len=MAX_LEN):\n",
    "    length = embedding_matrix.shape[0]\n",
    "\n",
    "    if length >= max_len:\n",
    "        return embedding_matrix[:max_len]\n",
    "\n",
    "    padding = np.zeros((max_len - length, EMBED_DIM))\n",
    "    return np.vstack([embedding_matrix, padding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecce6e-9562-4a71-8dca-91ca57607bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrgencyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, encoder):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        tokens = self.tokenizer(text)   # your tokeniser\n",
    "        embeddings = self.encoder.encode_tokens(tokens)\n",
    "        padded = pad_sequence(embeddings)\n",
    "\n",
    "        x = torch.tensor(padded, dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03f5df-1897-407c-8672-52f9b7dd0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrgencyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UrgencyModel, self).__init__()\n",
    "\n",
    "        # Parallel Conv1D layers\n",
    "        self.conv2 = nn.Conv1d(EMBED_DIM, 150, kernel_size=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(EMBED_DIM, 150, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(EMBED_DIM, 150, kernel_size=4, padding=2)\n",
    "\n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=450,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, L, D)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (B, D, L)\n",
    "\n",
    "        c2 = F.relu(self.conv2(x))\n",
    "        c3 = F.relu(self.conv3(x))\n",
    "        c4 = F.relu(self.conv4(x))\n",
    "\n",
    "        # Trim to same length\n",
    "        min_len = min(c2.shape[2], c3.shape[2], c4.shape[2])\n",
    "        c2 = c2[:, :, :min_len]\n",
    "        c3 = c3[:, :, :min_len]\n",
    "        c4 = c4[:, :, :min_len]\n",
    "\n",
    "        x = torch.cat([c2, c3, c4], dim=1)  # (B, 450, L)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (B, L, 450)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Take last time step\n",
    "        x = lstm_out[:, -1, :]  # (B, 256)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec003e-c342-4a8b-b3df-7e8bd7ba3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UrgencyModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0034298-f118-42a5-97e5-7f11401de5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf812bf-abcd-4940-8bc8-40d1cbe3a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"[caps on] i cannot believe this [multiple exclamations] where is my order [impatient questioning]\",\n",
    "    \"order delayed but not urgent\"\n",
    "]\n",
    "\n",
    "labels = [2, 1]  # Example: 0=Low, 1=Medium, 2=High\n",
    "\n",
    "# tokenizer = your_tokenizer_function\n",
    "# fasttext_model = load_fasttext_model()\n",
    "# encoder = FastTextEncoder(fasttext_model)\n",
    "\n",
    "# dataset = UrgencyDataset(texts, labels, tokenizer, encoder)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# train(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
